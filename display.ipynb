{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"display.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrsjTm/IgzyEnVhSBmxaH1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"jVyGqIrFgZxy"},"source":["from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torchvision.transforms as T\n","\n","\n","plt.rcParams[\"savefig.bbox\"] = 'tight'\n","\n","\n","def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):\n","    if not isinstance(imgs[0], list):\n","        # Make a 2d grid even if there's just 1 row\n","        imgs = [imgs]\n","\n","    num_rows = len(imgs)\n","    num_cols = len(imgs[0]) + with_orig\n","    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n","    for row_idx, row in enumerate(imgs):\n","        row = [orig_img] + row if with_orig else row\n","        for col_idx, img in enumerate(row):\n","            ax = axs[row_idx, col_idx]\n","            ax.imshow(np.asarray(img), **imshow_kwargs)\n","            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","    if with_orig:\n","        axs[0, 0].set(title='Original image')\n","        axs[0, 0].title.set_size(8)\n","    if row_title is not None:\n","        for row_idx in range(num_rows):\n","            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n","\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDVC3BaUgtUB","executionInfo":{"status":"ok","timestamp":1634361316883,"user_tz":-480,"elapsed":29218,"user":{"displayName":"苏志翔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06444298763113069178"}},"outputId":"76ba22f3-1100-4eb5-d393-7a75a515735c"},"source":["!git clone https://github.com/szx159753/FashionDataset.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'FashionDataset'...\n","remote: Enumerating objects: 14, done.\u001b[K\n","remote: Counting objects: 100% (14/14), done.\u001b[K\n","remote: Compressing objects: 100% (10/10), done.\u001b[K\n","remote: Total 14 (delta 2), reused 14 (delta 2), pack-reused 0\u001b[K\n","Unpacking objects: 100% (14/14), done.\n","Checking out files: 100% (6/6), done.\n"]}]},{"cell_type":"code","metadata":{"id":"WQq9yVK-g2U_"},"source":["!mv FashionDataset/focal_loss.py /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LH-_Y4mg4H_"},"source":["!tar xvf FashionDataset/data.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwhEVIkog7YH"},"source":["from __future__ import print_function, division\n","import os\n","\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","from collections import defaultdict\n","from torch.utils.data import DataLoader,Dataset\n","from torchvision.transforms import transforms\n","import subprocess\n","from focal_loss import FocalLoss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wi4-1Xmmg9IX"},"source":["data_dir='FashionDataset'\n","classes=[7,3,3,4,6,3]\n","# files={}\n","# labels={}\n","# for dir in ['train','val']:\n","#     files[dir]=open(os.path.join(data_dir,'split/'+dir+'.txt')).read().split('\\n')\n","#     labels[dir]=open(os.path.join(data_dir,'split/'+dir+'_attr.txt')).read().split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYKi4DJdhDH3","executionInfo":{"status":"ok","timestamp":1634383273184,"user_tz":-480,"elapsed":309,"user":{"displayName":"苏志翔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06444298763113069178"}}},"source":["class MyDataset(Dataset):\n","    def __init__(self,dir,y_label=False):\n","        self.transform = transforms.Compose([\n","            #transforms.RandomSizedCrop(224),\n","            transforms.Resize([224,224]),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        self.files=open(os.path.join(data_dir,'split/'+dir+'.txt')).read().split('\\n')[0:-1]\n","        self.len=len(self.files)\n","        self.y_label=y_label\n","\n","        bbox=open(os.path.join(data_dir,'split/'+dir+'_bbox.txt')).read().split('\\n')[0:-1]\n","        bbox=[bb.split(' ') for bb in bbox]\n","        self.bbox=bbox\n","        if self.y_label is True:\n","            labels=open(os.path.join(data_dir,'split/'+dir+'_attr.txt')).read().split('\\n')[0:-1]\n","            labels=[li.split(' ') for li in labels]\n","            self.labels=labels\n","    def __getitem__(self, idx):\n","        img_obj = Image.open(os.path.join(data_dir,self.files[idx]))\n","        bbox = np.array([int(l) for l in self.bbox[idx]],dtype=np.int)\n","        img_obj = img_obj.crop([bbox[0],bbox[1],bbox[2],bbox[3]])\n","        img_obj.save(\"img.jpg\")\n","        img_obj=self.transform(img_obj)\n","        if self.y_label is True:\n","            labels = np.array([int(l) for l in self.labels[idx]],dtype=np.float32)\n","            labels = torch.from_numpy(labels)\n","            return img_obj,labels\n","        else:\n","            return img_obj,self.files[idx]\n","    def __len__(self):\n","        return len(self.files)"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIAJjgADhFYn","executionInfo":{"status":"ok","timestamp":1634383275215,"user_tz":-480,"elapsed":711,"user":{"displayName":"苏志翔","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06444298763113069178"}}},"source":["image_datasets={x:MyDataset(x,y)\n","          for x,y in [['train',True],['val',True],['test',False]]}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n","                                             shuffle=y, num_workers=2)\n","              for x,y in [['train',False],['val',False],['test',False]]}\n","unloader = transforms.ToPILImage()\n","img=unloader(next(iter(dataloaders['train']))[0][6])\n","img.save(\"img.jpg\")"],"execution_count":110,"outputs":[]}]}